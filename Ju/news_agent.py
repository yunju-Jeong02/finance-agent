from utils import extract_date, is_url
from collections import Counter
import pandas as pd
import re

class NewsAgent:
    def __init__(self, clova_client, db_client, crawler, url_summarizer):
        self.clova = clova_client
        self.db = db_client
        self.crawler = crawler
        self.url_summarizer = url_summarizer

    def process_query(self, user_query):
    # URLì€ DB ê²€ìƒ‰ ê±´ë„ˆë›°ê³  ë°”ë¡œ ìš”ì•½
        if is_url(user_query):
            return f"[ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼]\n- ì˜ë„: url_summary_request\n- ë‚ ì§œ: ì—†ìŒ\n- í‚¤ì›Œë“œ: URL\n\n" + \
                self.url_summarizer.summarize_url(user_query)

        intent = self.clova.classify_intent(user_query)
        date = extract_date(user_query)
        keywords = self._extract_keyword(user_query)
        analysis_header = (
            f"[ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼]\n"
            f"- ì˜ë„(intent): {intent}\n"
            f"- ë‚ ì§œ: {date if date else 'ì—†ìŒ'}\n"
            f"- í‚¤ì›Œë“œ: {', '.join(keywords) if keywords else 'ì—†ìŒ'}\n\n"
        )

        # URLì€ DB íŒ¨ìŠ¤í•˜ê³  ë°”ë¡œ ìš”ì•½
        if intent == "url_summary_request":
            return analysis_header + self.url_summarizer.summarize_url(user_query)

        if intent == "clarification_needed":
            return analysis_header + "âš ï¸ ìš”ì²­ì´ ëª¨í˜¸í•©ë‹ˆë‹¤. í•˜ë‚˜ì”© ë¬¼ì–´ë´ ì£¼ì„¸ìš”."

        if intent == "today_news_request":
            return analysis_header + self._summarize_today_news(keywords)

        if intent == "news_summary_request":
            return analysis_header + self._summarize_keyword_news(keywords, date)

        if intent == "hot_news_request":
            return analysis_header + self._summarize_hot_news()

        return analysis_header + "â“ ìš”ì²­ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”."

    """
    def _summarize_today_news(self, keywords):
        query = " ".join(keywords)
        print(f"[INFO] Searching ê²½ì œ ë‰´ìŠ¤ for keywords: {query}")

        live_news = self.crawler.get_news_list(keyword=query)

        print(f"[INFO] Crawling result count: {len(live_news)}")
        if live_news:
            url = live_news[0]['link']
            print(f"[INFO] First news URL: {url}")
            return self.url_summarizer.summarize_url(url)

        return "â— ì˜¤ëŠ˜ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    """

    def _summarize_today_news(self, keywords):
        query = " ".join(keywords)
        print(f"[INFO] Searching news for keywords: {query}")

        live_news = self.crawler.get_all_news_by_date(query=query)

        print(f"[INFO] Crawling result count: {len(live_news)}")
        if live_news:
            url = live_news[0]['link']
            print(f"[INFO] First news URL: {url}")
            return self.url_summarizer.summarize_url(url)

        return "â— ì˜¤ëŠ˜ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


    def _summarize_keyword_news(self, keywords, date):
        results = self.db.search_news(keywords=keywords, date=date, limit=1)
        if not results:
            live = self.crawler.search_news(" ".join(keywords))
            if live:
                return self.url_summarizer.summarize_url(live[0]['link'])
            return "â— ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        news = results[0]
        return f"\nğŸ“Œ {news['title']}\nğŸ—“ {news['date']}\nğŸ”— {news['link']}\n\nğŸ“ ìš”ì•½:\n{self.clova.summarize(news.get('content') or news['title'])}"

    def _summarize_hot_news(self, limit=100):
        query = f"SELECT title, content FROM News ORDER BY date DESC LIMIT {limit}"
        try:
            df = pd.read_sql(query, con=self.db.engine)
        except Exception as e:
            return f"âŒ DB ì ‘ê·¼ ì˜¤ë¥˜: {e}"

        if df.empty:
            return "âŒ ìµœê·¼ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."

        text = ' '.join(df['title'])
        words = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', text).split()
        stopwords = {'ê·¸ë¦¬ê³ ','í•˜ì§€ë§Œ','ê·¸ë˜ì„œ','ë•Œë¬¸ì—','ìˆë‹¤','í•˜ë‹¤','ë˜ë‹¤','ì•Šë‹¤','ìˆ˜','ê²ƒ','ë“¤','ë“±'}
        counter = Counter([w for w in words if w not in stopwords and len(w) > 1])
        top_keywords = [w for w, _ in counter.most_common(5)]

        print("\nğŸ”¥ ìµœê·¼ ìì£¼ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ:")
        for i, kw in enumerate(top_keywords, 1):
            print(f"{i}. {kw}")
        choice = input("\nğŸ§‘ ìš”ì•½í•  í‚¤ì›Œë“œ ë²ˆí˜¸(1~5): ").strip()
        if not choice.isdigit() or not (1 <= int(choice) <= 5):
            return "â— ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤."

        selected = top_keywords[int(choice)-1]
        match = df[df['title'].str.contains(selected, na=False)]
        if match.empty:
            return f"âŒ {selected} ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."

        latest = match.iloc[0]
        summary = self.clova.summarize(latest.get('content') or latest['title'])
        return f"ğŸ“° ê¸°ì‚¬ ì œëª©: {latest['title']}\n\nğŸ“Œ ìš”ì•½:\n{summary}"

    def _extract_keyword(self, query):
        # ì¡°ì‚¬/ëª…ë ¹ì–´ ë“± ë¶ˆí•„ìš”í•œ ë‹¨ì–´ë“¤ ì œê±°
        stopwords = {"ìš”ì•½", "ë‰´ìŠ¤", "ì•Œë ¤ì¤˜", "í•´ì¤˜", "í•«í•œ", "ì‹¤ì‹œê°„", "ì˜¤ëŠ˜", "ìš”ì•½í•´ì¤˜"}
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ë‹¨ì–´ ë¶„ë¦¬
        words = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', query).split()
        # ë¶ˆìš©ì–´ ì œê±°
        keywords = [w for w in words if w not in stopwords]

        # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 'ë‰´ìŠ¤'
        return keywords
